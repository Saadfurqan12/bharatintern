
import pandas as pd
import numpy as np
Libraries for visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
import plotly.subplots as make_subplots
import datetime as dt
from sklearn.metrics import mean_squared_error , mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense , Dropout
from tensorflow.keras.layers import LSTM
from itertools import cycle
LOADING THE DATA
df=pd.read_csv('C:/Users/chand/Downloads/archive (2).zip')
df.set_index('Date',inplace=True)
df.head()
Open	High	Low	Close	Adj Close	Volume
Date						
1980-12-12	0.128348	0.128906	0.128348	0.128348	0.100323	469033600
1980-12-15	0.122210	0.122210	0.121652	0.121652	0.095089	175884800
1980-12-16	0.113281	0.113281	0.112723	0.112723	0.088110	105728000
1980-12-17	0.115513	0.116071	0.115513	0.115513	0.090291	86441600
1980-12-18	0.118862	0.119420	0.118862	0.118862	0.092908	73449600
print("Number of days present in the datset:",df.shape[0])
print("Number of fields in the dataset:",df.shape[1])
Number of days present in the datset: 10409
Number of fields in the dataset: 6
df.info()
<class 'pandas.core.frame.DataFrame'>
Index: 10409 entries, 1980-12-12 to 2022-03-24
Data columns (total 6 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   Open       10409 non-null  float64
 1   High       10409 non-null  float64
 2   Low        10409 non-null  float64
 3   Close      10409 non-null  float64
 4   Adj Close  10409 non-null  float64
 5   Volume     10409 non-null  int64  
dtypes: float64(5), int64(1)
memory usage: 569.2+ KB
df.describe()
Open	High	Low	Close	Adj Close	Volume
count	10409.000000	10409.000000	10409.000000	10409.000000	10409.000000	1.040900e+04
mean	13.959910	14.111936	13.809163	13.966757	13.350337	3.321778e+08
std	30.169244	30.514878	29.835055	30.191696	29.911132	3.393344e+08
min	0.049665	0.049665	0.049107	0.049107	0.038384	0.000000e+00
25%	0.281964	0.287946	0.274554	0.281250	0.234799	1.247604e+08
50%	0.468750	0.477679	0.459821	0.468750	0.386853	2.199680e+08
75%	14.217857	14.364286	14.043571	14.206071	12.188149	4.126108e+08
max	182.630005	182.940002	179.119995	182.009995	181.778397	7.421641e+09
from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)
check for null values
df.isnull().sum()
Open         0
High         0
Low          0
Close        0
Adj Close    0
Volume       0
dtype: int64
data plotting
data=df.iloc[2300:].copy()
plt.figure(figsize=(30,15))
ax=sns.lineplot(x=data.index,y=data['Close'])
plt.xticks(['20/08/2019','18/03/2020','29/02/2021','18/03/2022','10/01/2023'])
plt.show()

data=df.iloc[2300:].copy()
names=cycle(['Stock Open Price','Stock high price','Stock low price','Stock close price'])
fig=px.line(data,x=data.index, y=[data['Open'],data['High'],data['Low'],data['Close']],labels={'date': 'Date','value':'Stock value'})
fig.update_layout(title_text='ANALYSIS OF STOCKS', font_size=16,font_color='black',legend_title_text='STOCK PARAMETERS')
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=True)
fig.for_each_trace(lambda t: t.update(name=next(names)))
fig.show()
Moving Averages
ma_day=[30,60,120,150]
for ma in ma_day:
    column_name=f'MA for {ma} days'
    data[column_name]=data['Close'].rolling(ma).mean()
plt.figure(figsize=(35,20))
plt.plot(data['Close'],label='Close Price')
plt.plot(data['MA for 30 days'],label='30 days MA')
plt.plot(data['MA for 60 days'],label='60 days MA')
plt.plot(data['MA for 120 days'],label='120 days MA')
plt.plot(data['MA for 150 days'],label='150 days MA')
plt.xticks(['20/08/2019','18/03/2020','29/02/2021','18/03/2022','10/01/2023'])
plt.legend()
plt.show()

names=cycle(['Close Price', 'MA for 30 days','MA for 60 days','MA for 120 days','MA for 150 days'])
fig = px.line(data, 
              x=data.index,
              y=[data['Close'], data['MA for 30 days'], data['MA for 60 days'], data['MA for 120 days'], data['MA for 150 days']],
              labels={'x': 'Date', 'y': 'Values'})

fig.update_layout(title_text='Moving Average Analysis', font_size=15, font_color='black', legend_title_text='Stock Parameters')
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=False)
fig.for_each_trace(lambda t: t.update(name=next(names)))
fig.show()
SPLITTING THE TIME-SERIES DATA
new_df=data['Close']
new_df.index=data.index
final_df=new_df.values
train_data=final_df[0:646,]
test_data=final_df[646:,]
train_df=pd.DataFrame()
test_df=pd.DataFrame()
train_df['Close']=train_data
train_df.index=new_df[0:646].index
test_df['Close']=test_data
test_df.index=new_df[646:].index
print('train_data:',train_df.shape)
print('test_data:',test_df.shape)
train_data: (646, 1)
test_data: (7463, 1)
SCALING USING MIN-MAX SCALAR
scaler=MinMaxScaler(feature_range=(0,1))
scaled_data=scaler.fit_transform(final_df.reshape(-1,1))
X_train_data,y_train_data=[],[]
for i in range(60,len(train_df)):
    X_train_data.append(scaled_data[i-60:i,0])
    y_train_data.append(scaled_data[i,0])
X_train_data,y_train_data=np.array(X_train_data),np.array(y_train_data)
X_train_data=np.reshape(X_train_data,(X_train_data.shape[0],X_train_data.shape[1],1))
MODEL BUILDING USING LSTM MODEL
#initializing the LSTM model
model=Sequential()
model.add(LSTM(units=50,return_sequences=True, input_shape= (X_train_data.shape[1],1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50,return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50,return_sequences=True))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 60, 50)            10400     
                                                                 
 dropout (Dropout)           (None, 60, 50)            0         
                                                                 
 lstm_1 (LSTM)               (None, 60, 50)            20200     
                                                                 
 dropout_1 (Dropout)         (None, 60, 50)            0         
                                                                 
 lstm_2 (LSTM)               (None, 60, 50)            20200     
                                                                 
 lstm_3 (LSTM)               (None, 50)                20200     
                                                                 
 dropout_2 (Dropout)         (None, 50)                0         
                                                                 
 dense (Dense)               (None, 1)                 51        
                                                                 
=================================================================
Total params: 71051 (277.54 KB)
Trainable params: 71051 (277.54 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train_data,y_train_data,epochs=150,batch_size=32)
Epoch 1/150
19/19 [==============================] - 6s 51ms/step - loss: 8.0311e-05
Epoch 2/150
19/19 [==============================] - 1s 52ms/step - loss: 5.7128e-06
Epoch 3/150
19/19 [==============================] - 1s 49ms/step - loss: 1.5125e-06
Epoch 4/150
19/19 [==============================] - 1s 50ms/step - loss: 5.2411e-07
Epoch 5/150
19/19 [==============================] - 1s 49ms/step - loss: 3.2087e-07
Epoch 6/150
19/19 [==============================] - 1s 48ms/step - loss: 3.2623e-07
Epoch 7/150
19/19 [==============================] - 1s 49ms/step - loss: 2.8139e-07
Epoch 8/150
19/19 [==============================] - 1s 47ms/step - loss: 2.9152e-07
Epoch 9/150
19/19 [==============================] - 1s 49ms/step - loss: 2.8017e-07
Epoch 10/150
19/19 [==============================] - 1s 53ms/step - loss: 2.9058e-07
Epoch 11/150
19/19 [==============================] - 1s 47ms/step - loss: 2.6803e-07
Epoch 12/150
19/19 [==============================] - 1s 48ms/step - loss: 2.8013e-07
Epoch 13/150
19/19 [==============================] - 1s 48ms/step - loss: 2.9751e-07
Epoch 14/150
19/19 [==============================] - 1s 48ms/step - loss: 2.9156e-07
Epoch 15/150
19/19 [==============================] - 1s 48ms/step - loss: 3.0508e-07
Epoch 16/150
19/19 [==============================] - 1s 56ms/step - loss: 3.0619e-07
Epoch 17/150
19/19 [==============================] - 1s 56ms/step - loss: 2.8867e-07
Epoch 18/150
19/19 [==============================] - 1s 59ms/step - loss: 2.9436e-07
Epoch 19/150
19/19 [==============================] - 1s 57ms/step - loss: 2.9121e-07
Epoch 20/150
19/19 [==============================] - 1s 57ms/step - loss: 3.1620e-07
Epoch 21/150
19/19 [==============================] - 1s 56ms/step - loss: 3.0808e-07
Epoch 22/150
19/19 [==============================] - 1s 57ms/step - loss: 2.9754e-07
Epoch 23/150
19/19 [==============================] - 1s 56ms/step - loss: 2.6992e-07
Epoch 24/150
19/19 [==============================] - 1s 56ms/step - loss: 2.8171e-07
Epoch 25/150
19/19 [==============================] - 1s 56ms/step - loss: 2.9657e-07
Epoch 26/150
19/19 [==============================] - 1s 58ms/step - loss: 2.8428e-07
Epoch 27/150
19/19 [==============================] - 1s 56ms/step - loss: 3.2810e-07
Epoch 28/150
19/19 [==============================] - 1s 56ms/step - loss: 2.9037e-07
Epoch 29/150
19/19 [==============================] - 1s 57ms/step - loss: 2.9911e-07
Epoch 30/150
19/19 [==============================] - 1s 57ms/step - loss: 3.4474e-07
Epoch 31/150
19/19 [==============================] - 1s 57ms/step - loss: 3.0697e-07
Epoch 32/150
19/19 [==============================] - 1s 66ms/step - loss: 3.1523e-07
Epoch 33/150
19/19 [==============================] - 1s 55ms/step - loss: 3.0425e-07
Epoch 34/150
19/19 [==============================] - 1s 56ms/step - loss: 3.1870e-07
Epoch 35/150
19/19 [==============================] - 1s 68ms/step - loss: 3.0239e-07
Epoch 36/150
19/19 [==============================] - 1s 70ms/step - loss: 3.0862e-07
Epoch 37/150
19/19 [==============================] - 1s 58ms/step - loss: 3.0428e-07
Epoch 38/150
19/19 [==============================] - 1s 56ms/step - loss: 2.9125e-07
Epoch 39/150
19/19 [==============================] - 1s 56ms/step - loss: 2.9242e-07
Epoch 40/150
19/19 [==============================] - 1s 56ms/step - loss: 2.8908e-07
Epoch 41/150
19/19 [==============================] - 1s 55ms/step - loss: 2.9598e-07
Epoch 42/150
19/19 [==============================] - 1s 54ms/step - loss: 2.8945e-07
Epoch 43/150
19/19 [==============================] - 1s 57ms/step - loss: 2.8630e-07
Epoch 44/150
19/19 [==============================] - 1s 54ms/step - loss: 2.8386e-07
Epoch 45/150
19/19 [==============================] - 1s 56ms/step - loss: 2.9347e-07
Epoch 46/150
19/19 [==============================] - 1s 55ms/step - loss: 3.4701e-07
Epoch 47/150
19/19 [==============================] - 1s 56ms/step - loss: 3.2162e-07
Epoch 48/150
19/19 [==============================] - 1s 57ms/step - loss: 2.9395e-07
Epoch 49/150
19/19 [==============================] - 1s 56ms/step - loss: 3.0072e-07
Epoch 50/150
19/19 [==============================] - 1s 57ms/step - loss: 3.1945e-07
Epoch 51/150
19/19 [==============================] - 1s 63ms/step - loss: 3.4372e-07
Epoch 52/150
19/19 [==============================] - 1s 57ms/step - loss: 2.8877e-07
Epoch 53/150
19/19 [==============================] - 1s 56ms/step - loss: 2.5578e-07
Epoch 54/150
19/19 [==============================] - 1s 57ms/step - loss: 2.5380e-07
Epoch 55/150
19/19 [==============================] - 1s 56ms/step - loss: 2.7595e-07
Epoch 56/150
19/19 [==============================] - 1s 56ms/step - loss: 2.7129e-07
Epoch 57/150
19/19 [==============================] - 1s 58ms/step - loss: 3.0404e-07
Epoch 58/150
19/19 [==============================] - 1s 56ms/step - loss: 3.3740e-07
Epoch 59/150
19/19 [==============================] - 1s 58ms/step - loss: 3.4585e-07
Epoch 60/150
19/19 [==============================] - 1s 62ms/step - loss: 2.7780e-07
Epoch 61/150
19/19 [==============================] - 1s 57ms/step - loss: 2.5949e-07
Epoch 62/150
19/19 [==============================] - 1s 56ms/step - loss: 3.2003e-07
Epoch 63/150
19/19 [==============================] - 1s 57ms/step - loss: 2.8590e-07
Epoch 64/150
19/19 [==============================] - 1s 62ms/step - loss: 2.4264e-07
Epoch 65/150
19/19 [==============================] - 1s 62ms/step - loss: 2.8077e-07
Epoch 66/150
19/19 [==============================] - 1s 67ms/step - loss: 3.2018e-07
Epoch 67/150
19/19 [==============================] - 1s 63ms/step - loss: 2.6231e-07
Epoch 68/150
19/19 [==============================] - 1s 63ms/step - loss: 2.9603e-07
Epoch 69/150
19/19 [==============================] - 1s 69ms/step - loss: 2.4448e-07
Epoch 70/150
19/19 [==============================] - 1s 56ms/step - loss: 2.3811e-07
Epoch 71/150
19/19 [==============================] - 1s 59ms/step - loss: 2.9079e-07
Epoch 72/150
19/19 [==============================] - 1s 64ms/step - loss: 3.6467e-07
Epoch 73/150
19/19 [==============================] - 1s 56ms/step - loss: 3.4481e-07
Epoch 74/150
19/19 [==============================] - 1s 55ms/step - loss: 2.6489e-07
Epoch 75/150
19/19 [==============================] - 1s 56ms/step - loss: 3.4929e-07
Epoch 76/150
19/19 [==============================] - 1s 56ms/step - loss: 3.2444e-07
Epoch 77/150
19/19 [==============================] - 1s 56ms/step - loss: 3.4650e-07
Epoch 78/150
19/19 [==============================] - 1s 57ms/step - loss: 3.9260e-07
Epoch 79/150
19/19 [==============================] - 1s 56ms/step - loss: 3.1573e-07
Epoch 80/150
19/19 [==============================] - 1s 55ms/step - loss: 2.6887e-07
Epoch 81/150
19/19 [==============================] - 1s 56ms/step - loss: 2.3485e-07
Epoch 82/150
19/19 [==============================] - 1s 57ms/step - loss: 3.1008e-07
Epoch 83/150
19/19 [==============================] - 1s 57ms/step - loss: 2.9442e-07
Epoch 84/150
19/19 [==============================] - 1s 56ms/step - loss: 2.7718e-07
Epoch 85/150
19/19 [==============================] - 1s 56ms/step - loss: 2.4642e-07
Epoch 86/150
19/19 [==============================] - 1s 59ms/step - loss: 1.9945e-07
Epoch 87/150
19/19 [==============================] - 1s 59ms/step - loss: 2.4086e-07
Epoch 88/150
19/19 [==============================] - 1s 54ms/step - loss: 2.0533e-07
Epoch 89/150
19/19 [==============================] - 1s 50ms/step - loss: 1.7137e-07
Epoch 90/150
19/19 [==============================] - 1s 49ms/step - loss: 2.7365e-07
Epoch 91/150
19/19 [==============================] - 1s 49ms/step - loss: 2.3299e-07
Epoch 92/150
19/19 [==============================] - 1s 52ms/step - loss: 2.0624e-07
Epoch 93/150
19/19 [==============================] - 1s 49ms/step - loss: 4.5204e-07
Epoch 94/150
19/19 [==============================] - 1s 51ms/step - loss: 3.2798e-07
Epoch 95/150
19/19 [==============================] - 1s 49ms/step - loss: 2.5345e-07
Epoch 96/150
19/19 [==============================] - 1s 49ms/step - loss: 3.1198e-07
Epoch 97/150
19/19 [==============================] - 1s 50ms/step - loss: 1.9756e-07
Epoch 98/150
19/19 [==============================] - 1s 49ms/step - loss: 2.3296e-07
Epoch 99/150
19/19 [==============================] - 1s 50ms/step - loss: 3.2363e-07
Epoch 100/150
19/19 [==============================] - 1s 50ms/step - loss: 1.9995e-07
Epoch 101/150
19/19 [==============================] - 1s 49ms/step - loss: 3.1260e-07
Epoch 102/150
19/19 [==============================] - 1s 49ms/step - loss: 3.2514e-07
Epoch 103/150
19/19 [==============================] - 1s 50ms/step - loss: 4.1857e-07
Epoch 104/150
19/19 [==============================] - 1s 53ms/step - loss: 6.3818e-07
Epoch 105/150
19/19 [==============================] - 1s 51ms/step - loss: 6.8708e-07
Epoch 106/150
19/19 [==============================] - 1s 50ms/step - loss: 3.1434e-07
Epoch 107/150
19/19 [==============================] - 1s 49ms/step - loss: 2.3887e-07
Epoch 108/150
19/19 [==============================] - 1s 48ms/step - loss: 2.2569e-07
Epoch 109/150
19/19 [==============================] - 1s 49ms/step - loss: 2.8638e-07
Epoch 110/150
19/19 [==============================] - 1s 48ms/step - loss: 4.7605e-07
Epoch 111/150
19/19 [==============================] - 1s 49ms/step - loss: 3.0724e-07
Epoch 112/150
19/19 [==============================] - 1s 49ms/step - loss: 3.2276e-07
Epoch 113/150
19/19 [==============================] - 1s 49ms/step - loss: 2.3635e-07
Epoch 114/150
19/19 [==============================] - 1s 53ms/step - loss: 2.9177e-07
Epoch 115/150
19/19 [==============================] - 1s 50ms/step - loss: 2.4120e-07
Epoch 116/150
19/19 [==============================] - 1s 48ms/step - loss: 2.0743e-07
Epoch 117/150
19/19 [==============================] - 1s 48ms/step - loss: 3.8057e-07
Epoch 118/150
19/19 [==============================] - 1s 49ms/step - loss: 2.4260e-07
Epoch 119/150
19/19 [==============================] - 1s 49ms/step - loss: 1.6536e-07
Epoch 120/150
19/19 [==============================] - 1s 49ms/step - loss: 1.7532e-07
Epoch 121/150
19/19 [==============================] - 1s 49ms/step - loss: 1.8956e-07
Epoch 122/150
19/19 [==============================] - 1s 51ms/step - loss: 3.1746e-07
Epoch 123/150
19/19 [==============================] - 1s 49ms/step - loss: 1.8781e-07
Epoch 124/150
19/19 [==============================] - 1s 49ms/step - loss: 1.5739e-07
Epoch 125/150
19/19 [==============================] - 1s 48ms/step - loss: 2.5636e-07
Epoch 126/150
19/19 [==============================] - 1s 49ms/step - loss: 2.3932e-07
Epoch 127/150
19/19 [==============================] - 1s 49ms/step - loss: 4.0354e-07
Epoch 128/150
19/19 [==============================] - 1s 50ms/step - loss: 4.7549e-07
Epoch 129/150
19/19 [==============================] - 1s 49ms/step - loss: 2.2350e-07
Epoch 130/150
19/19 [==============================] - 1s 51ms/step - loss: 2.3803e-07
Epoch 131/150
19/19 [==============================] - 1s 49ms/step - loss: 5.7041e-07
Epoch 132/150
19/19 [==============================] - 1s 49ms/step - loss: 6.2546e-07
Epoch 133/150
19/19 [==============================] - 1s 50ms/step - loss: 4.7189e-07
Epoch 134/150
19/19 [==============================] - 1s 49ms/step - loss: 4.2789e-07
Epoch 135/150
19/19 [==============================] - 1s 49ms/step - loss: 2.4253e-07
Epoch 136/150
19/19 [==============================] - 1s 49ms/step - loss: 2.2874e-07
Epoch 137/150
19/19 [==============================] - 1s 50ms/step - loss: 2.1400e-07
Epoch 138/150
19/19 [==============================] - 1s 48ms/step - loss: 2.3754e-07
Epoch 139/150
19/19 [==============================] - 1s 48ms/step - loss: 1.7730e-07
Epoch 140/150
19/19 [==============================] - 1s 49ms/step - loss: 1.5296e-07
Epoch 141/150
19/19 [==============================] - 1s 50ms/step - loss: 3.2489e-07
Epoch 142/150
19/19 [==============================] - 1s 48ms/step - loss: 1.9328e-07
Epoch 143/150
19/19 [==============================] - 1s 49ms/step - loss: 2.4735e-07
Epoch 144/150
19/19 [==============================] - 1s 49ms/step - loss: 2.4443e-07
Epoch 145/150
19/19 [==============================] - 1s 51ms/step - loss: 2.3749e-07
Epoch 146/150
19/19 [==============================] - 1s 48ms/step - loss: 3.1515e-07
Epoch 147/150
19/19 [==============================] - 1s 49ms/step - loss: 1.6513e-07
Epoch 148/150
19/19 [==============================] - 1s 49ms/step - loss: 1.6658e-07
Epoch 149/150
19/19 [==============================] - 1s 49ms/step - loss: 2.0279e-07
Epoch 150/150
19/19 [==============================] - 1s 49ms/step - loss: 2.2451e-07
<keras.src.callbacks.History at 0x1c57e29af50>
PREDICTIONS
input_data=new_df[len(new_df)-len(test_df)-60:].values
input_data=input_data.reshape(-1,1)
input_data=scaler.transform(input_data)
X_test=[]
for i in range(60,input_data.shape[0]):
    X_test.append(input_data[i-60:i,0])
X_test=np.array(X_test)
X_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))
predicted=model.predict(X_test)
predicted=scaler.inverse_transform(predicted)
234/234 [==============================] - 5s 16ms/step
test_df['Predictions']=predicted
plt.figure(figsize=(50,10))
plt.plot(train_df['Close'],label='Training Data')
plt.plot(test_df['Close'],label='Test Data')
plt.plot(test_df['Predictions'], label='Prediction')
plt.xticks(['20/08/2019','18/03/2020','29/02/2021','18/03/2022','10/01/2023'])
plt.legend()
plt.show()

fig=go.Figure()
fig.add_trace(go.Scatter(x=train_df.index,y=train_df['Close'],
                         mode='lines',
                         name='Training Data'))
fig.add_trace(go.Scatter(x=test_df.index,y=test_df['Close'],
                         mode='lines',
                        name='Test Data'))
fig.add_trace(go.Scatter(x=test_df.index,y=test_df['Predictions'],
                         mode='lines',
                         name='Prediction'))
print('The Mean Squarred Error is',mean_squared_error(test_df['Close'].values,test_df['Predictions'].values))
print('The Mean Absolute Error is',mean_absolute_error(test_df['Close'].values,test_df['Predictions'].values))
print('The Root Mean Squarred Error is',np.sqrt(mean_squared_error(test_df['Close'].values,test_df['Predictions'].values)))
The Mean Squarred Error is 738.4816206763247
The Mean Absolute Error is 11.380179629381075
The Root Mean Squarred Error is 27.175018319705412
